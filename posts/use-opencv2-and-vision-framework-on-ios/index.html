<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  






  
  
  
    
    
  





  



  
  

<link rel="stylesheet" href="https://ai-doge.github.io/main.min.css" />


  
<meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
    <meta name="bingbot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
  

<title>Use Opencv2 and Vision framework on iOS</title>


<meta name="author" content="ai-doge">

<meta name="description" content="Ai-Doge&#39;s Blog">
<link rel="canonical" href="https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/">
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Use Opencv2 and Vision framework on iOS">
<meta property="og:description" content="Sharing Our Experience in Developing FaceRefresh Link to this heading Meet the App: Before the Code Link to this heading Your memories are unique, priceless, and yours alone. As life moves on, it&rsquo;s these moments that matter most. FaceRefresh: AI Photo Restorer is here to bring them back to life, vividly and securely, right on your device.
How to Use OpenCV2 in a Swift Project Link to this heading It&rsquo;s worth mentioning that our initial use of OpenCV was somewhat of a detour.">
<meta property="og:url" content="https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/">
<meta property="article:published_time" content="2023-09-25T13:54:33+08:00">
  <meta property="article:modified_time" content="2023-09-25T13:54:33+08:00">
  


  <meta name="og:image" content="https://ai-doge.github.io/images/ai-doge.jpg"/>






  <meta name="twitter:site" content="aidogeshow">


  <meta name="twitter:creator" content="aidogeshow">

<meta name="twitter:title" content="Use Opencv2 and Vision framework on iOS">
<meta name="twitter:description" content="Sharing Our Experience in Developing FaceRefresh Link to this heading Meet the App: Before the Code Link to this heading Your memories are unique, priceless, and yours alone. As life moves on, it&rsquo;s these moments that matter most. FaceRefresh: AI Photo Restorer is here to bring them back to life, vividly and securely, right on your device.
How to Use OpenCV2 in a Swift Project Link to this heading It&rsquo;s worth mentioning that our initial use of OpenCV was somewhat of a detour.">



  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:image" content="https://ai-doge.github.io/images/ai-doge.jpg"/>





  


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@graph": [
    {
      "@type": "Person",
      "@id": "https://ai-doge.github.io/#/schema/person/1",
      "name": "Ai Doge",
      "url": "https://ai-doge.github.io/",
      "image": {
        "@type": "ImageObject",
        "@id": "https://ai-doge.github.io/#/schema/image/1",
        "url": "https://ai-doge.github.io/images/ai-doge.jpg",
        "width": 453 ,
        "height": 455 ,
        "caption": "Ai Doge"
      }
    },
    {
      "@type": "WebSite",
      "@id": "https://ai-doge.github.io/#/schema/website/1",
      "url": "https://ai-doge.github.io/",
      "name": "ai-doge",
      "description": "Ai-Doge's Blog",
      "publisher": {
        "@id": "https://ai-doge.github.io/#/schema/person/1"
      }
    },
    {
      "@type": "WebPage",
      "@id": "https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/",
      "url": "https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/",
      "name": "Using OpenCV2 and Vision framework on iOS",
      "description": "Ai-Doge's Blog",
      "isPartOf": {
        "@id": "https://ai-doge.github.io/#/schema/website/1"
      },
      "about": {
        "@id": "https://ai-doge.github.io/#/schema/person/1"
      },
      "datePublished": "2023-09-25T13:54:33+08:00",
      "dateModified": "2023-09-25T13:54:33+08:00",
      "breadcrumb": {
        "@id": "https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/#/schema/breadcrumb/1"
      },
      "primaryImageOfPage": {
        "@id": "https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/#/schema/image/2"
      },
      "inLanguage": "en-US",
      "potentialAction": [{
        "@type": "ReadAction", "target": ["https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/"]
      }]
    },
    {
      "@type": "BreadcrumbList",
      "@id": "https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/#/schema/breadcrumb/1",
      "name": "Breadcrumbs",
      "itemListElement": [{
        "@type": "ListItem",
        "position":  1 ,
        "item": {
          "@type": "WebPage",
          "@id": "https://ai-doge.github.io/",
          "url": "https://ai-doge.github.io/",
          "name": "Home"
          }
        },{
        "@type": "ListItem",
        "position":  2 ,
        "item": {
          "@type": "WebPage",
          "@id": "https://ai-doge.github.io/posts/",
          "url": "https://ai-doge.github.io/posts/",
          "name": "Posts"
          }
        },{
        "@type": "ListItem",
        "position":  3 ,
        "item": {
          "@id": "https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/"
          }
        }]
    },
    {
      "@context": "https://schema.org",
      "@graph": [
        {
          "@type": "Article",
          "@id": "https://ai-doge.github.io/#/schema/article/1",
          "headline": "Using OpenCV2 and Vision framework on iOS",
          "description": "",
          "isPartOf": {
            "@id": "https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/"
          },
          "mainEntityOfPage": {
            "@id": "https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/"
          },
          "datePublished": "2023-09-25T13:54:33+08:00",
          "dateModified": "2023-09-25T13:54:33+08:00",
          "author": {
            "@id": "https://ai-doge.github.io/#/schema/person/1"
          },          
          "publisher": {
            "@id": "https://ai-doge.github.io/#/schema/person/1"
          },
          "image": {
            "@id": "https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/#/schema/image/2"
          }
        }
      ]
    },{
      "@context": "https://schema.org",
      "@graph": [
        {
          "@type": "ImageObject",
          "@id": "https://ai-doge.github.io/posts/use-opencv2-and-vision-framework-on-ios/#/schema/image/2",
          "url": "https://ai-doge.github.io/images/ai-doge.jpg",
          "contentUrl": "https://ai-doge.github.io/images/ai-doge.jpg",
          "caption": "Using OpenCV2 and Vision framework on iOS"
        }
      ]
    }
  ]
}
</script>
  

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">

  

</head><body>
    <header class="container">
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WMMNNH5CCF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WMMNNH5CCF');
</script>

  <nav class="main-nav" id="js-navbar">
    <a class="logo" href="https://ai-doge.github.io/">ai-doge</a>
    <ul class="menu" id="js-menu">
      
      
      
      <li class="menu-item">
        <span class="menu-link">Products<span class="drop-icon">▾</span></span>
        <ul class="sub-menu">
          
            <li class="menu-item">
              <a href="/projects/" class="menu-link">Products</a>                  
            </li>
          
            <li class="menu-item">
              <a href="/about/" class="menu-link">About</a>                  
            </li>
          
        </ul>
      </li>
      
      
      
      <li class="menu-item">
        <span class="menu-link">Posts<span class="drop-icon">▾</span></span>
        <ul class="sub-menu">
          
            <li class="menu-item">
              <a href="/posts/" class="menu-link">All Posts</a>                  
            </li>
          
        </ul>
      </li>
      
      
      <li class="menu-item--align">
        <div class="switch">
          <input class="switch-input" type="checkbox" id="themeSwitch">
          <label aria-hidden="true" class="switch-label" for="themeSwitch">On</label>
          <div aria-hidden="true" class="switch-marker"></div>
        </div>
      </li>
    </ul>
    <span class="nav-toggle" id="js-navbar-toggle">
      <svg xmlns="http://www.w3.org/2000/svg" id="Outline" viewBox="0 0 24 24" width="30" height="30" fill="var(--color-contrast-high)"><rect y="11" width="24" height="2" rx="1"/><rect y="4" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg>
    </span>
  </nav>
</header><main class="section">
<div class="container">
  <section class="page-header">
    <h1 class="page-header-title">Using OpenCV2 and Vision framework on iOS</h1>
    <div class="post-list-meta">
      <div class="post-list-dates">Sep 25, 2023&nbsp;&middot;&nbsp;3 min.</div>
      
      <div class="post-list-categories">
        
          <a href="https://ai-doge.github.io/categories/opencv/">OpenCV</a>
        
      </div>
      
      
    </div>
    <p class="page-header-desc"></p>
    <div class="single-terms">
      
      
      <a class="term" href="https://ai-doge.github.io/tags/opencv/">OpenCV</a></li>
      
      <a class="term" href="https://ai-doge.github.io/tags/super-resolution/">Super Resolution</a></li>
      
      
    </div>
  </section>
</div>
<div class="single-container-post">
  

  <aside class="toc">
    <div id="js-toc-toggle">
      <h2 class="toc-header">Table of Contents</h2>
      <span class="toc-drop-icon">&blacktriangledown;</span>
    </div>
    <div id="js-toc-contents" class="toc-contents"><nav id="TableOfContents">
  <ul>
    <li><a href="#meet-the-app-before-the-code">Meet the App: Before the Code</a></li>
    <li><a href="#how-to-use-opencv2-in-a-swift-project">How to Use OpenCV2 in a Swift Project</a></li>
    <li><a href="#things-to-be-cautious-when-using-ios-vision-framework-for-facial-landmark-detection">Things to Be Cautious When Using iOS Vision Framework for Facial Landmark Detection</a>
      <ul>
        <li><a href="#vision-framework-cannot-be-used-for-facial-landmark-detection-on-simulators">Vision Framework Cannot Be Used for Facial Landmark Detection on Simulators</a></li>
        <li><a href="#choosing-between-constellation65points-and-constellation76points">Choosing between constellation65Points and constellation76Points</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
  </aside>

  <div class="single-post-contents">
    <div class="single-feature-img">



  

</div>
    <article class="markdown">
        <h1 id="sharing-our-experience-in-developing-facerefresh">Sharing Our Experience in Developing FaceRefresh<a href="#sharing-our-experience-in-developing-facerefresh">
    <svg role="img" aria-labelledby="sharing-our-experience-in-developing-facerefresh-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="sharing-our-experience-in-developing-facerefresh-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h1><h2 id="meet-the-app-before-the-code">Meet the App: Before the Code<a href="#meet-the-app-before-the-code">
    <svg role="img" aria-labelledby="meet-the-app-before-the-code-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="meet-the-app-before-the-code-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h2><p>Your memories are unique, priceless, and yours alone. As life moves on, it&rsquo;s these moments that matter most.
<a href="https://apps.apple.com/app/facerefresh-ai-photo-restorer/id6463642671" target="_blank" rel="noopener">FaceRefresh: AI Photo Restorer</a> is here to bring them back to life, vividly and securely, right on your device.</p>
<h2 id="how-to-use-opencv2-in-a-swift-project">How to Use OpenCV2 in a Swift Project<a href="#how-to-use-opencv2-in-a-swift-project">
    <svg role="img" aria-labelledby="how-to-use-opencv2-in-a-swift-project-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="how-to-use-opencv2-in-a-swift-project-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h2><p>It&rsquo;s worth mentioning that our initial use of OpenCV was somewhat of a detour.</p>
<p>Firstly we downloaded the OpenCV2 framework (version 4.8) for iOS from the <a href="https://opencv.org/releases/" target="_blank" rel="noopener">official website</a>,</p>
<p>Then we followed <a href="https://medium.com/pharos-production/using-opencv-in-a-swift-project-679868e1b798" target="_blank" rel="noopener">a tutorial online</a>.
According to the tutorial we needed to configure Xcode and implement wrappers for the OpenCV2 functions we wanted to use. Despite following the tutorial step by step, we encountered various linking issues and couldn&rsquo;t resolve them even after checking every possible error point.</p>
<p>Later, we accidentally discovered that OpenCV2 already contains Swift code. So, theoretically, we shouldn&rsquo;t need to write custom wrappers. After some tinkering, we found that using it is actually quite simple. <strong>All we needed to do was</strong>:</p>
<ol>
<li>Add OpenCV2 as a framework dependency in the project.</li>
<li>Configure the Framework Search Paths in Target &gt; Build Settings to include the path where opencv.framework is located.</li>
</ol>
<p>Here is a screenshot of the first step:

	
		
	
	
	<img src="/posts/use-opencv2-and-vision-framework-on-ios/images/import_opencv2_to_project_1_2_hu68dbb8e94dc0440135817def803ab714_250605_800x0_resize_q75_h2_box_3.webp"
	width="800"
	height="242"
	
	alt="import_opencv2_to_project_1" 
	class="single-post-image" 
	loading="lazy"
	decoding="async"
	>
</p>
<p>And here is a screenshot of the second step (the opencv.framework directory is placed under the libs folder):

	
		
	
	
	<img src="/posts/use-opencv2-and-vision-framework-on-ios/images/import_opencv2_to_project_3_huacee8fbbcfde6f89664f19c5bb43ff37_287415_800x0_resize_q75_h2_box_3.webp"
	width="800"
	height="335"
	
	alt="import_opencv2_to_project_2" 
	class="single-post-image" 
	loading="lazy"
	decoding="async"
	>
</p>
<p>One issue we encountered was that the package names for OpenCV in Python and Swift don&rsquo;t match. For example, in Python, we would use cv2.estimateAffinePartial2D and cv2.warpAffine, but in Swift, we should use Calib3d.estimateAffinePartial2D and Imgproc.warpAffine. The only workaround we found was to use grep within the downloaded opencv.framework.</p>
<p>Lastly, one inconvenience is that the framework downloaded from the official website cannot be compiled on a simulator; you have to compile it locally.</p>
<h2 id="things-to-be-cautious-when-using-ios-vision-framework-for-facial-landmark-detection">Things to Be Cautious When Using iOS Vision Framework for Facial Landmark Detection<a href="#things-to-be-cautious-when-using-ios-vision-framework-for-facial-landmark-detection">
    <svg role="img" aria-labelledby="things-to-be-cautious-when-using-ios-vision-framework-for-facial-landmark-detection-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="things-to-be-cautious-when-using-ios-vision-framework-for-facial-landmark-detection-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h2><h3 id="vision-framework-cannot-be-used-for-facial-landmark-detection-on-simulators">Vision Framework Cannot Be Used for Facial Landmark Detection on Simulators<a href="#vision-framework-cannot-be-used-for-facial-landmark-detection-on-simulators">
    <svg role="img" aria-labelledby="vision-framework-cannot-be-used-for-facial-landmark-detection-on-simulators-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="vision-framework-cannot-be-used-for-facial-landmark-detection-on-simulators-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h3><p>When developing the FaceRefresh app, it were our first time using the Vision framework. We weren&rsquo;t aware that:</p>
<ol>
<li>The coordinate system returned by all APIs is different from the display coordinate system on the iPhone (the y-axis is flipped).</li>
<li>Facial landmark detection can only be done on a real device.
Initially, we encountered an error:</li>
</ol>


  <pre><code>Error Domain=com.apple.Vision Code=9 
&#34;encountered an unexpected condition: Unspecified error&#34; 
UserInfo={NSLocalizedDescription=encountered an 
          unexpected condition: Unspecified error}</code></pre>
<p>However, we found a switch (landmarksRequest.usesCPUOnly = true) that allowed the code to run, but the detected landmarks were incorrect.</p>


  <span class="code-language">swift</span><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-swift" data-lang="swift"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>    <span style="color:#66d9ef">let</span> landmarksRequest = VNDetectFaceLandmarksRequest()
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span>    landmarksRequest.constellation = VNRequestFaceLandmarksConstellation.constellation76Points
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3</span><span>    landmarksRequest.revision = VNDetectFaceLandmarksRequestRevision3
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5</span><span>    <span style="color:#75715e">// use cpu here</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6</span><span>    landmarksRequest.usesCPUOnly = <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">7</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">8</span><span>    <span style="color:#66d9ef">let</span> requestHandler = VNImageRequestHandler(ciImage: image, options: [:])</span></span></code></pre></div>
<p>Here are the results when running on a simulator (on an M1 MacBook):

	
		
	
	
	
		
	<img src="/posts/use-opencv2-and-vision-framework-on-ios/images/2_face_landmarks_constellation_on_simulator_hu415ed2a3b76a4b45357dd8f7776c07e5_337678_800x0_resize_q75_h2_box.webp"
	width="800"
	height="402"
	style="
		background-image: url('data:image/jpg;base64,/9j/2wCEACgcHiMeGSgjISMtKygwPGRBPDc3PHtYXUlkkYCZlo&#43;AjIqgtObDoKrarYqMyP/L2u71////m8H////6/&#43;b9//gBKy0tPDU8dkFBdviljKX4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;P/AABEIABgAMAMBIgACEQEDEQH/xAGiAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgsQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5&#43;gEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoLEQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4&#43;Tl5ufo6ery8/T19vf4&#43;fr/2gAMAwEAAhEDEQA/AEIPmP8AWp02Mmwnmq0rFXYDqafbDauRgnvU20NmE6sMIGGWPQdhU4u4xjr&#43;VVoQWdn654FII2weKaM5lv7am/aOc1RnZrm4YFyFzgYqW1UYwQD71XDeXOXbOAc8UrhYfK2LhhT7ZcphicGop/8Aj4f/AD2qe3/1a03saLcegEDFc8LzUpl3oSBxUNz/AKxv90UsX&#43;p/CpZLQ6BCob3qGS1YyEtjb14qzD0p0v3W&#43;lIGf//Z');
		background-position: 50% 50%;
		background-repeat: no-repeat;
		background-size: cover;"
	alt="face_landmarks_constellation_on_simulator" 
	class="single-post-image" 
	loading="lazy"
	decoding="async"
	>
</p>
<p>And here are the results on a real device (also with useCPUOnly = true enabled):

	
		
	
	
	
		
	<img src="/posts/use-opencv2-and-vision-framework-on-ios/images/2_face_landmarks_constellation_76_on_device_hu415ed2a3b76a4b45357dd8f7776c07e5_388742_800x0_resize_q75_h2_box.webp"
	width="800"
	height="400"
	style="
		background-image: url('data:image/jpg;base64,/9j/2wCEACgcHiMeGSgjISMtKygwPGRBPDc3PHtYXUlkkYCZlo&#43;AjIqgtObDoKrarYqMyP/L2u71////m8H////6/&#43;b9//gBKy0tPDU8dkFBdviljKX4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;P/AABEIABgAMAMBIgACEQEDEQH/xAGiAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgsQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5&#43;gEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoLEQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4&#43;Tl5ufo6ery8/T19vf4&#43;fr/2gAMAwEAAhEDEQA/AEIPmP8AWp0MbJsJyaqysVdgvUmpLbCLkcnvU20NmE6kYRXyzHoOwqYXaDHBqvCN0jP17CgRNjpTRnMsC&#43;UuFAzmqcxNzcsGZtucADtUtqoA24B9c1Xz5c7OegPQGlcLD5WxcOKfar8mGJwain/4&#43;Hqe3/1YpvY0Q9FEDld3C81J5pkUkDiorn/WN/uilg/1R&#43;lSyWOgTGeetRSWv7wsxGKsQ0tx0akhM//Z');
		background-position: 50% 50%;
		background-repeat: no-repeat;
		background-size: cover;"
	alt="face_landmarks_constellation_76_on_device" 
	class="single-post-image" 
	loading="lazy"
	decoding="async"
	>
</p>
<p>Both issues combined led to a lot of debugging time. Initially, we realized that we had made mistake #1 and corrected it. However, at that time, we thought there might be other issues similar to mistake #1. It turned out that it was actually mistake #2 causing the problems.</p>
<h3 id="choosing-between-constellation65points-and-constellation76points">Choosing between constellation65Points and constellation76Points<a href="#choosing-between-constellation65points-and-constellation76points">
    <svg role="img" aria-labelledby="choosing-between-constellation65points-and-constellation76points-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="choosing-between-constellation65points-and-constellation76points-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h3><p>In FaceRefresh App, we only needed to detect a few key points like the pupils, nose tip, and mouth corners. Therefore, both constellation65Points and constellation76Points would work for me. However, after testing with a Lenna image, we found that constellation76Points gave more accurate results, which was not mentioned in the documentation.</p>
<p>Here are the results using constellation65Points:

	
		
	
	
	
		
	<img src="/posts/use-opencv2-and-vision-framework-on-ios/images/face_landmarks_constellation_65_hu415ed2a3b76a4b45357dd8f7776c07e5_422419_800x0_resize_q75_h2_box.webp"
	width="800"
	height="400"
	style="
		background-image: url('data:image/jpg;base64,/9j/2wCEACgcHiMeGSgjISMtKygwPGRBPDc3PHtYXUlkkYCZlo&#43;AjIqgtObDoKrarYqMyP/L2u71////m8H////6/&#43;b9//gBKy0tPDU8dkFBdviljKX4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;P/AABEIABgAMAMBIgACEQEDEQH/xAGiAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgsQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5&#43;gEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoLEQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4&#43;Tl5ufo6ery8/T19vf4&#43;fr/2gAMAwEAAhEDEQA/AGpnAq0oVo9rVSaTZHgdetS264Id8kn9Kmxsx7o0aMxI2gce5qNQePcU2Zmln2ZO3NNkkIUKvUcUWBMvYR49p6&#43;tV51aKJm3DPAUUtuojPIJPc1CCZbgZJ2jnmmLUj4KlzzjpU1u5ZMc5qBf9UfpU1n/AFpyBBFne7OMH3qE/Km4dasN/rJfrVdv9V&#43;NCAsxMWiBUEkDgUyDhSW65qSz&#43;6KYOjf71SNn/9k=');
		background-position: 50% 50%;
		background-repeat: no-repeat;
		background-size: cover;"
	alt="face_landmarks_constellation_65_on_device" 
	class="single-post-image" 
	loading="lazy"
	decoding="async"
	>
</p>
<p>And here are the results using constellation76Points:</p>
<p>
	
		
	
	
	
		
	<img src="/posts/use-opencv2-and-vision-framework-on-ios/images/face_landmarks_constellation_76_hu415ed2a3b76a4b45357dd8f7776c07e5_419652_800x0_resize_q75_h2_box.webp"
	width="800"
	height="400"
	style="
		background-image: url('data:image/jpg;base64,/9j/2wCEACgcHiMeGSgjISMtKygwPGRBPDc3PHtYXUlkkYCZlo&#43;AjIqgtObDoKrarYqMyP/L2u71////m8H////6/&#43;b9//gBKy0tPDU8dkFBdviljKX4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;Pj4&#43;P/AABEIABgAMAMBIgACEQEDEQH/xAGiAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgsQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5&#43;gEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoLEQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4&#43;Tl5ufo6ery8/T19vf4&#43;fr/2gAMAwEAAhEDEQA/AGpnAq0oVo9rVSaTZHgdetS264IZ8kn9Kmxsx7o0aMxI2gce5qNM8U2dmlm2ZO3NNkl2qAOoosCZdKK8eCeexqCXfFGz5AxjbS242H5yS38qh3NNOATlRzRYWpHwVLnnHSp7dyyYquv&#43;qNTWf9aqQIIiS7swwfeoT8q7&#43;9Tt/rJfrUD/AOqoQFmJyYsgE4FMt/ulj1NOtf8AV/hTI/ufjUjZ/9k=');
		background-position: 50% 50%;
		background-repeat: no-repeat;
		background-size: cover;"
	alt="face_landmarks_constellation_76_on_device" 
	class="single-post-image" 
	loading="lazy"
	decoding="async"
	>
</p>
<p>Compared to constellation65Points, constellation76Points provided more accurate pupil and mouth corners positions.</p>

    </article>
    <aside>
      <div class="single-terms">
        
          
          <a class="term" href="https://ai-doge.github.io/tags/opencv/">OpenCV</a></li>
          
          <a class="term" href="https://ai-doge.github.io/tags/super-resolution/">Super Resolution</a></li>
          
        
      </div>
      
  
  
  

  <section>
    <h2>Share</h2>
    <div class="social-links">
      <ul class="social-icons--share">
        
        
        <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2fai-doge.github.io%2fposts%2fuse-opencv2-and-vision-framework-on-ios%2f&amp;text=Using%20OpenCV2%20and%20Vision%20framework%20on%20iOS" target="_blank" rel="noopener" aria-label="Share on Twitter" class="social-btn twitter">
          <li><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-twitter" width="24" height="24" viewBox="0 0 384 312" fill="var(--color-primary)"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5 0-78.8 35.3-78.8 78.8 0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3-6.7 11.6-10.6 25.2-10.6 39.6 0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1 0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4 0-12.6-.4-18.8-1.1 34.9 22.4 76.3 35.4 120.8 35.4 144.9 0 224.1-120 224.1-224.1 0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg></li>
        </a>
        
        
        
        <a href="https://www.reddit.com/submit?url=https%3a%2f%2fai-doge.github.io%2fposts%2fuse-opencv2-and-vision-framework-on-ios%2f" target="_blank" rel="noopener" aria-label="Share on Reddit" class="social-btn reddit">
          <li><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-reddit" width="24" height="24" viewBox="0 0 24 24" fill="var(--color-primary)"><path d="M24 11.779c0-1.459-1.192-2.645-2.657-2.645-.715 0-1.363.286-1.84.746-1.81-1.191-4.259-1.949-6.971-2.046l1.483-4.669 4.016.941-.006.058c0 1.193.975 2.163 2.174 2.163 1.198 0 2.172-.97 2.172-2.163s-.975-2.164-2.172-2.164c-.92 0-1.704.574-2.021 1.379l-4.329-1.015c-.189-.046-.381.063-.44.249l-1.654 5.207c-2.838.034-5.409.798-7.3 2.025-.474-.438-1.103-.712-1.799-.712-1.465 0-2.656 1.187-2.656 2.646 0 .97.533 1.811 1.317 2.271-.052.282-.086.567-.086.857 0 3.911 4.808 7.093 10.719 7.093s10.72-3.182 10.72-7.093c0-.274-.029-.544-.075-.81.832-.447 1.405-1.312 1.405-2.318zm-17.224 1.816c0-.868.71-1.575 1.582-1.575.872 0 1.581.707 1.581 1.575s-.709 1.574-1.581 1.574-1.582-.706-1.582-1.574zm9.061 4.669c-.797.793-2.048 1.179-3.824 1.179l-.013-.003-.013.003c-1.777 0-3.028-.386-3.824-1.179-.145-.144-.145-.379 0-.523.145-.145.381-.145.526 0 .65.647 1.729.961 3.298.961l.013.003.013-.003c1.569 0 2.648-.315 3.298-.962.145-.145.381-.144.526 0 .145.145.145.379 0 .524zm-.189-3.095c-.872 0-1.581-.706-1.581-1.574 0-.868.709-1.575 1.581-1.575s1.581.707 1.581 1.575-.709 1.574-1.581 1.574z"/></svg></li>
        </a>
        
        
                
        <a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fai-doge.github.io%2fposts%2fuse-opencv2-and-vision-framework-on-ios%2f" target="_blank" rel="noopener" aria-label="Share on Facebook" class="social-btn facebook">
          <li><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-facebook" width="24" height="24" viewBox="0 0 352 352" fill="var(--color-primary)"><path d="m0 32v288c0 17.5 14.5 32 32 32h288c17.5 0 32-14.5 32-32v-288c0-17.5-14.5-32-32-32h-288c-17.5 0-32 14.5-32 32zm320 0v288h-83v-108h41.5l6-48h-47.5v-31c0-14 3.5-23.5 23.5-23.5h26v-43.5c-4.4-.6-19.8-1.5-37.5-1.5-36.9 0-62 22.2-62 63.5v36h-42v48h42v108h-155v-288z"/></svg></li>
        </a>
        
        
        
        <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fai-doge.github.io%2fposts%2fuse-opencv2-and-vision-framework-on-ios%2f&amp;source=https%3a%2f%2fai-doge.github.io%2fposts%2fuse-opencv2-and-vision-framework-on-ios%2f&amp;title=Using%20OpenCV2%20and%20Vision%20framework%20on%20iOS&amp;summary=Using%20OpenCV2%20and%20Vision%20framework%20on%20iOS" target="_blank" rel="noopener" aria-label="Share on LinkedIn" class="social-btn linkedin">
          <li><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-linkedin" width="24" height="24" viewBox="0 0 352 352" fill="var(--color-primary)"><path d="M0,40v272c0,21.9,18.1,40,40,40h272c21.9,0,40-18.1,40-40V40c0-21.9-18.1-40-40-40H40C18.1,0,0,18.1,0,40z M312,32 c4.6,0,8,3.4,8,8v272c0,4.6-3.4,8-8,8H40c-4.6,0-8-3.4-8-8V40c0-4.6,3.4-8,8-8H312z M59.5,87c0,15.2,12.3,27.5,27.5,27.5 c15.2,0,27.5-12.3,27.5-27.5c0-15.2-12.3-27.5-27.5-27.5C71.8,59.5,59.5,71.8,59.5,87z M187,157h-1v-21h-45v152h47v-75 c0-19.8,3.9-39,28.5-39c24.2,0,24.5,22.4,24.5,40v74h47v-83.5c0-40.9-8.7-72-56.5-72C208.5,132.5,193.3,145.1,187,157z M64,288h47.5 V136H64V288z"/></svg></li>
        </a>
        
        
        
        <a href="mailto:?subject=ai-doge%20-%20Using%20OpenCV2%20and%20Vision%20framework%20on%20iOS.&amp;body=Using%20OpenCV2%20and%20Vision%20framework%20on%20iOS%2c%20by%20ai-doge%0a%0a%0ahttps%3a%2f%2fai-doge.github.io%2fposts%2fuse-opencv2-and-vision-framework-on-ios%2f%0a" target="_blank" class="social-btn email">
          <li><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-mail" width="24" height="24" viewBox="0 0 416 288" fill="var(--color-primary)"><path d="m0 16v256 16h16 384 16v-16-256-16h-16-384-16zm347 16-139 92.5-139-92.5zm-148 125.5 9 5.5 9-5.5 167-111.5v210h-352v-210z"/></svg></li>
        </a>
      </ul>
    </div>
  </section>
  
        

  
  

        
  <section>
    <h2>Read Next</h2>
    <div class="single-next-previous">
      
        <a class="previous" href="https://ai-doge.github.io/posts/segment_anything/">&laquo; Porting the Segment Anything Model to iOS</a>
      
      
    </div>
  </section>

      
    </aside>
  </div>
</div>

    </main><footer>
  
  <div class="section footer">
    <p class="footer-copyright">&copy; 2023 &middot; 
      <a href="https://ai-doge.github.io/">ai-doge</a>
      
        <span>&middot; Built with <a href="https://github.com/wjh18/hugo-liftoff" target="_blank" rel="noopener">Hugo Liftoff</a> theme.</span>
      
    </p>
    
      <div class="footer-socials">
        
<div class="social-links">
  <ul class="social-icons">
    
    
    <li>
      <a href="https://twitter.com/aidogeshow" target="_blank" rel="noopener" aria-label="Visit Twitter profile" class="social-btn twitter">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-twitter" width="24" height="24" viewBox="0 0 384 312" fill="var(--color-primary)"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5 0-78.8 35.3-78.8 78.8 0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3-6.7 11.6-10.6 25.2-10.6 39.6 0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1 0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4 0-12.6-.4-18.8-1.1 34.9 22.4 76.3 35.4 120.8 35.4 144.9 0 224.1-120 224.1-224.1 0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
      </a>
    </li>
    

    
    

    
    
    <li>
      <a href="https://github.com/ai-doge" target="_blank" rel="noopener" aria-label="Visit Github profile" class="social-btn github">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-github" width="24" height="24" viewBox="0 0 24 24" fill="var(--color-primary)"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
      </a>
    </li>
    

    
    

    
    

    
    
  </ul>
</div>

      </div>
    
  </div>
</footer>

  





  
  
  
    
    
  




  
  
    

<script src="https://ai-doge.github.io/main.min.js"></script>



  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WMMNNH5CCF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WMMNNH5CCF', { 'anonymize_ip': false });
}
</script>

</body>
</html>
