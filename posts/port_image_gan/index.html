<!DOCTYPE html>
<html lang="en-us"><head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  
  






  
  
  
    
    
  





  



  
  

<link rel="stylesheet" href="https://ai-doge.github.io/main.min.css" />


  
<meta name="robots" content="index, follow">
    <meta name="googlebot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
    <meta name="bingbot" content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1">
  

<title>Implementing ImageGAN</title>


<meta name="author" content="ai-doge">

<meta name="description" content="How to train a projected gan on landscapes/ clouds dataset, then port it on iPhone.">
<link rel="canonical" href="https://ai-doge.github.io/posts/port_image_gan/">
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Implementing ImageGAN">
<meta property="og:description" content="How to train a projected gan on landscapes/ clouds dataset, then port it on iPhone.">
<meta property="og:url" content="https://ai-doge.github.io/posts/port_image_gan/">
<meta property="article:published_time" content="2023-08-10T11:43:24+08:00">
  <meta property="article:modified_time" content="2023-08-10T11:43:24+08:00">
  


  <meta name="og:image" content="https://ai-doge.github.io/images/ai-doge.jpg"/>






  <meta name="twitter:site" content="aidogeshow">


  <meta name="twitter:creator" content="aidogeshow">

<meta name="twitter:title" content="Implementing ImageGAN">
<meta name="twitter:description" content="How to train a projected gan on landscapes/ clouds dataset, then port it on iPhone.">



  <meta name="twitter:card" content="summary_large_image"/>
  <meta name="twitter:image" content="https://ai-doge.github.io/images/ai-doge.jpg"/>





  


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@graph": [
    {
      "@type": "Person",
      "@id": "https://ai-doge.github.io/#/schema/person/1",
      "name": "Ai Doge",
      "url": "https://ai-doge.github.io/",
      "image": {
        "@type": "ImageObject",
        "@id": "https://ai-doge.github.io/#/schema/image/1",
        "url": "https://ai-doge.github.io/images/ai-doge.jpg",
        "width": 453 ,
        "height": 455 ,
        "caption": "Ai Doge"
      }
    },
    {
      "@type": "WebSite",
      "@id": "https://ai-doge.github.io/#/schema/website/1",
      "url": "https://ai-doge.github.io/",
      "name": "ai-doge",
      "description": "Ai-Doge's Blog",
      "publisher": {
        "@id": "https://ai-doge.github.io/#/schema/person/1"
      }
    },
    {
      "@type": "WebPage",
      "@id": "https://ai-doge.github.io/posts/port_image_gan/",
      "url": "https://ai-doge.github.io/posts/port_image_gan/",
      "name": "Implementing ImageGAN From Data Collection to Model Compression",
      "description": "How to train a projected gan on landscapes/ clouds dataset, then port it on iPhone.",
      "isPartOf": {
        "@id": "https://ai-doge.github.io/#/schema/website/1"
      },
      "about": {
        "@id": "https://ai-doge.github.io/#/schema/person/1"
      },
      "datePublished": "2023-08-10T11:43:24+08:00",
      "dateModified": "2023-08-10T11:43:24+08:00",
      "breadcrumb": {
        "@id": "https://ai-doge.github.io/posts/port_image_gan/#/schema/breadcrumb/1"
      },
      "primaryImageOfPage": {
        "@id": "https://ai-doge.github.io/posts/port_image_gan/#/schema/image/2"
      },
      "inLanguage": "en-US",
      "potentialAction": [{
        "@type": "ReadAction", "target": ["https://ai-doge.github.io/posts/port_image_gan/"]
      }]
    },
    {
      "@type": "BreadcrumbList",
      "@id": "https://ai-doge.github.io/posts/port_image_gan/#/schema/breadcrumb/1",
      "name": "Breadcrumbs",
      "itemListElement": [{
        "@type": "ListItem",
        "position":  1 ,
        "item": {
          "@type": "WebPage",
          "@id": "https://ai-doge.github.io/",
          "url": "https://ai-doge.github.io/",
          "name": "Home"
          }
        },{
        "@type": "ListItem",
        "position":  2 ,
        "item": {
          "@type": "WebPage",
          "@id": "https://ai-doge.github.io/posts/",
          "url": "https://ai-doge.github.io/posts/",
          "name": "Posts"
          }
        },{
        "@type": "ListItem",
        "position":  3 ,
        "item": {
          "@id": "https://ai-doge.github.io/posts/port_image_gan/"
          }
        }]
    },
    {
      "@context": "https://schema.org",
      "@graph": [
        {
          "@type": "Article",
          "@id": "https://ai-doge.github.io/#/schema/article/1",
          "headline": "Implementing ImageGAN From Data Collection to Model Compression",
          "description": "How to train a projected gan on landscapes/ clouds dataset, then port it on iPhone.",
          "isPartOf": {
            "@id": "https://ai-doge.github.io/posts/port_image_gan/"
          },
          "mainEntityOfPage": {
            "@id": "https://ai-doge.github.io/posts/port_image_gan/"
          },
          "datePublished": "2023-08-10T11:43:24+08:00",
          "dateModified": "2023-08-10T11:43:24+08:00",
          "author": {
            "@id": "https://ai-doge.github.io/#/schema/person/1"
          },          
          "publisher": {
            "@id": "https://ai-doge.github.io/#/schema/person/1"
          },
          "image": {
            "@id": "https://ai-doge.github.io/posts/port_image_gan/#/schema/image/2"
          }
        }
      ]
    },{
      "@context": "https://schema.org",
      "@graph": [
        {
          "@type": "ImageObject",
          "@id": "https://ai-doge.github.io/posts/port_image_gan/#/schema/image/2",
          "url": "https://ai-doge.github.io/images/ai-doge.jpg",
          "contentUrl": "https://ai-doge.github.io/images/ai-doge.jpg",
          "caption": "Implementing ImageGAN From Data Collection to Model Compression"
        }
      ]
    }
  ]
}
</script>
  

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">

  

</head><body>
    <header class="container">
  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WMMNNH5CCF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WMMNNH5CCF');
</script>

  <nav class="main-nav" id="js-navbar">
    <a class="logo" href="https://ai-doge.github.io/">ai-doge</a>
    <ul class="menu" id="js-menu">
      
      
      
      <li class="menu-item">
        <span class="menu-link">Products<span class="drop-icon">▾</span></span>
        <ul class="sub-menu">
          
            <li class="menu-item">
              <a href="/projects/" class="menu-link">Products</a>                  
            </li>
          
            <li class="menu-item">
              <a href="/about/" class="menu-link">About</a>                  
            </li>
          
        </ul>
      </li>
      
      
      
      <li class="menu-item">
        <span class="menu-link">Posts<span class="drop-icon">▾</span></span>
        <ul class="sub-menu">
          
            <li class="menu-item">
              <a href="/posts/" class="menu-link">All Posts</a>                  
            </li>
          
        </ul>
      </li>
      
      
      <li class="menu-item--align">
        <div class="switch">
          <input class="switch-input" type="checkbox" id="themeSwitch">
          <label aria-hidden="true" class="switch-label" for="themeSwitch">On</label>
          <div aria-hidden="true" class="switch-marker"></div>
        </div>
      </li>
    </ul>
    <span class="nav-toggle" id="js-navbar-toggle">
      <svg xmlns="http://www.w3.org/2000/svg" id="Outline" viewBox="0 0 24 24" width="30" height="30" fill="var(--color-contrast-high)"><rect y="11" width="24" height="2" rx="1"/><rect y="4" width="24" height="2" rx="1"/><rect y="18" width="24" height="2" rx="1"/></svg>
    </span>
  </nav>
</header><main class="section">
<div class="container">
  <section class="page-header">
    <h1 class="page-header-title">Implementing ImageGAN From Data Collection to Model Compression</h1>
    <div class="post-list-meta">
      <div class="post-list-dates">Aug 10, 2023&nbsp;&middot;&nbsp;5 min.</div>
      
      <div class="post-list-categories">
        
          <a href="https://ai-doge.github.io/categories/image-gan/">Image GAN</a>
        
          <a href="https://ai-doge.github.io/categories/colab/">Colab</a>
        
      </div>
      
      
    </div>
    <p class="page-header-desc">How to train a projected gan on landscapes/ clouds dataset, then port it on iPhone.</p>
    <div class="single-terms">
      
      
      <a class="term" href="https://ai-doge.github.io/tags/image-gan/">Image GAN</a></li>
      
      <a class="term" href="https://ai-doge.github.io/tags/colab-training/">Colab Training</a></li>
      
      <a class="term" href="https://ai-doge.github.io/tags/kmeans-weight-quant/">KMeans Weight Quant</a></li>
      
      
    </div>
  </section>
</div>
<div class="single-container-post">
  

  <aside class="toc">
    <div id="js-toc-toggle">
      <h2 class="toc-header">Table of Contents</h2>
      <span class="toc-drop-icon">&blacktriangledown;</span>
    </div>
    <div id="js-toc-contents" class="toc-contents"><nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#preparing-training-data">Preparing Training Data</a>
      <ul>
        <li><a href="#data-collection-landscapes">Data Collection: Landscapes</a></li>
        <li><a href="#data-preprocessing">Data Preprocessing</a></li>
      </ul>
    </li>
    <li><a href="#training-the-projected-gan-model">Training the Projected GAN Model</a>
      <ul>
        <li><a href="#choosing-an-open-source-project">Choosing an Open-Source Project</a></li>
        <li><a href="#model-customization-for-mobile-deployment">Model Customization for Mobile Deployment</a></li>
      </ul>
    </li>
    <li><a href="#exporting-the-trained-model-to-onnx-and-optimization">Exporting the Trained Model to ONNX and Optimization</a>
      <ul>
        <li><a href="#exporting-to-onnx-format">Exporting to ONNX Format</a></li>
        <li><a href="#model-simplification-using-onnxsim">Model Simplification: Using onnxsim</a></li>
        <li><a href="#model-compression-quantization">Model Compression: Quantization</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav></div>
  </aside>

  <div class="single-post-contents">
    <div class="single-feature-img">



  

</div>
    <article class="markdown">
        <h2 id="introduction">Introduction<a href="#introduction">
    <svg role="img" aria-labelledby="introduction-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="introduction-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h2><p><a href="https://apps.apple.com/us/app/image-gan/id1637359196" target="_blank" rel="noopener">Image GAN</a> is an innovative application that merges art and technology to create unique visuals. Utilizing advanced GAN (Generative Adversarial Network) algorithms, the app offers a range of features:</p>
<ul>
<li><strong>AI Landscapes</strong>: Explore AI-generated landscapes that capture the beauty of nature in a novel way.</li>
<li><strong>Dynamic Cloudscapes</strong>: Experience ever-changing cloud patterns, curated solely by artificial intelligence.</li>
<li><strong>AI Masterpieces</strong>: Witness artistry as AI channels the essence of legendary painters to create new masterworks.</li>
</ul>
<p>For those interested in the technical aspects, this blog post will walk you through the process of implementing ImageGAN, focusing on data preparation and model training.</p>
<h2 id="preparing-training-data">Preparing Training Data<a href="#preparing-training-data">
    <svg role="img" aria-labelledby="preparing-training-data-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="preparing-training-data-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h2><h3 id="data-collection-landscapes">Data Collection: Landscapes<a href="#data-collection-landscapes">
    <svg role="img" aria-labelledby="data-collection-landscapes-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="data-collection-landscapes-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h3><p>To train our GAN model, we needed a dataset that was both high-quality and relevant to our application&rsquo;s focus on landscapes. We used web scraping techniques to collect free and commercially usable natural landscape photos. If you&rsquo;re interested, you can download the dataset from <a href="https://drive.google.com/file/d/1zmHJS8DIqQ7vJmIFH4xoBYL7zHGi2-XO/view" target="_blank" rel="noopener">this Google Drive link</a>.</p>
<h3 id="data-preprocessing">Data Preprocessing<a href="#data-preprocessing">
    <svg role="img" aria-labelledby="data-preprocessing-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="data-preprocessing-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h3><p>The collected images were preprocessed to fit the requirements of GAN training. Specifically, we resized the images to a uniform 512x512 pixel resolution. This step is crucial for the stability and effectiveness of the GAN model.</p>
<h2 id="training-the-projected-gan-model">Training the Projected GAN Model<a href="#training-the-projected-gan-model">
    <svg role="img" aria-labelledby="training-the-projected-gan-model-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="training-the-projected-gan-model-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h2><h3 id="choosing-an-open-source-project">Choosing an Open-Source Project<a href="#choosing-an-open-source-project">
    <svg role="img" aria-labelledby="choosing-an-open-source-project-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="choosing-an-open-source-project-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h3><p>For the model training, we chose the open-source project <a href="https://github.com/autonomousvision/projected-gan" target="_blank" rel="noopener">Projected GAN</a> as our base code. Projected GAN is an exceptional work that allows for rapid training convergence. It was presented in a NeurIPS 2021 paper titled &ldquo;Projected GANs Converge Faster&rdquo; by Axel Sauer, Kashyap Chitta, Jens Müller, and Andreas Geiger. The repository also provides a quick start Colab notebook for those interested in trying it out.</p>
<h3 id="model-customization-for-mobile-deployment">Model Customization for Mobile Deployment<a href="#model-customization-for-mobile-deployment">
    <svg role="img" aria-labelledby="model-customization-for-mobile-deployment-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="model-customization-for-mobile-deployment-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h3><p>To ensure that the trained model could be efficiently deployed on mobile devices, we made some modifications to the model architecture. Specifically, we opted for the <code>fastgan_lite</code> model, which is a relatively lightweight version of the original model.</p>
<h4 id="code-modifications">Code Modifications<a href="#code-modifications">
    <svg role="img" aria-labelledby="code-modifications-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="code-modifications-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h4><p>We modified the <code>FastganSynthesis</code> class in the generator to adjust the <code>ngf</code> parameter from 128 to 64. This change effectively halved the size of the trained model without compromising the quality of generated images in our application.</p>
<p>Here&rsquo;s a snippet of the modified code:</p>


  <span class="code-language">python</span><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FastganSynthesis</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span>    <span style="color:#66d9ef">def</span> __init__(self, ngf<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, z_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, nc<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, img_resolution<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, lite<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">3</span><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">4</span><span>        self<span style="color:#f92672">.</span>img_resolution <span style="color:#f92672">=</span> img_resolution
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">5</span><span>        self<span style="color:#f92672">.</span>z_dim <span style="color:#f92672">=</span> z_dim
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">6</span><span>        <span style="color:#75715e"># ... (rest of the code remains the same)</span></span></span></code></pre></div>
<p>By making these adjustments, we were able to train a model that not only converges quickly but is also optimized for mobile deployment.</p>
<h2 id="exporting-the-trained-model-to-onnx-and-optimization">Exporting the Trained Model to ONNX and Optimization<a href="#exporting-the-trained-model-to-onnx-and-optimization">
    <svg role="img" aria-labelledby="exporting-the-trained-model-to-onnx-and-optimization-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="exporting-the-trained-model-to-onnx-and-optimization-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h2><p>After the model training is complete, the next step is to export the model to ONNX (Open Neural Network Exchange) format so that it can run on different platforms and environments. This section will detail how to go about this process, with special emphasis on two key steps: model simplification and model compression.</p>
<h3 id="exporting-to-onnx-format">Exporting to ONNX Format<a href="#exporting-to-onnx-format">
    <svg role="img" aria-labelledby="exporting-to-onnx-format-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="exporting-to-onnx-format-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h3><p>First, we use PyTorch&rsquo;s <code>torch.onnx.export</code> function to export the trained GAN model to ONNX format. Let&rsquo;s assume the exported model file is named <code>gan.onnx</code>.</p>


  <span class="code-language">python</span><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1</span><span><span style="color:#f92672">import</span> torch.onnx
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2</span><span><span style="color:#f92672">import</span> torchvision.models <span style="color:#66d9ef">as</span> models
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4</span><span><span style="color:#75715e">### Initialize the model and input</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5</span><span>model <span style="color:#f92672">=</span> YourTrainedGANModel()
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6</span><span>x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(batch_size, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">512</span>, requires_grad<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7</span><span>torch_out <span style="color:#f92672">=</span> model(x)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9</span><span><span style="color:#75715e">### Export the model</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10</span><span>torch<span style="color:#f92672">.</span>onnx<span style="color:#f92672">.</span>export(model,                     <span style="color:#75715e"># model being run</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11</span><span>                  x,                         <span style="color:#75715e"># model input</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12</span><span>                  <span style="color:#e6db74">&#34;gan.onnx&#34;</span>,                <span style="color:#75715e"># where to save the model</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">13</span><span>                  export_params<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)        <span style="color:#75715e"># store the trained parameter weights inside the model file</span></span></span></code></pre></div>
<h3 id="model-simplification-using-onnxsim">Model Simplification: Using onnxsim<a href="#model-simplification-using-onnxsim">
    <svg role="img" aria-labelledby="model-simplification-using-onnxsim-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="model-simplification-using-onnxsim-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h3><p>Once the model is exported, we use the <code>onnxsim</code> tool to simplify the model. This step usually removes many redundant layers, thereby accelerating model inference and reducing the layers that need to be supported on the device.</p>


  <span class="code-language">bash</span><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">1</span><span>pip install onnxsim
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">2</span><span>onnxsim gan.onnx gan.sim.onnx</span></span></code></pre></div>
<h3 id="model-compression-quantization">Model Compression: Quantization<a href="#model-compression-quantization">
    <svg role="img" aria-labelledby="model-compression-quantization-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="model-compression-quantization-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h3><h4 id="using-int8-precision">Using INT8 Precision<a href="#using-int8-precision">
    <svg role="img" aria-labelledby="using-int8-precision-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="using-int8-precision-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h4><p>The original FP32 model size is close to 100MB, which is too large for a device-side application. Therefore, we plan to use INT8 precision to compress the model, which usually reduces the model size to a quarter of the original.</p>
<h4 id="k-means-quantization">K-means Quantization<a href="#k-means-quantization">
    <svg role="img" aria-labelledby="k-means-quantization-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="k-means-quantization-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h4><p>In GAN models, traditional quantization methods like scale symmetric quantization or scale zeroPoint asymmetric quantization may lead to significant model accuracy loss. To address this, we use the K-means algorithm for quantization.</p>
<p>Specifically, for each well-trained weight matrix in the network, we use the K-means algorithm to cluster its values into 256 classes. Then, we record the class centers and the class IDs corresponding to each weight.</p>
<p>This K-means quantization method has been tested to almost completely retain the original network&rsquo;s generated image quality. The Cosine Similarity with the original FP32 model&rsquo;s output results is above 0.999.</p>


  <span class="code-language">python</span><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 1</span><span><span style="color:#75715e"># K-means Quantization Code Example (Pseudocode)</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 2</span><span><span style="color:#f92672">from</span> sklearn.cluster <span style="color:#f92672">import</span> KMeans
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 3</span><span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 4</span><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kmeans_quantize</span>(weight_matrix):
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 5</span><span>    original_shape <span style="color:#f92672">=</span> weight_matrix<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 6</span><span>    weight_matrix <span style="color:#f92672">=</span> weight_matrix<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 7</span><span>    kmeans <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>fit(weight_matrix)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 8</span><span>    centers <span style="color:#f92672">=</span> kmeans<span style="color:#f92672">.</span>cluster_centers_
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f"> 9</span><span>    labels <span style="color:#f92672">=</span> kmeans<span style="color:#f92672">.</span>labels_
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">10</span><span>    <span style="color:#75715e"># Replace the original weights with the class centers</span>
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">11</span><span>    quantized_matrix <span style="color:#f92672">=</span> centers[labels]<span style="color:#f92672">.</span>reshape(original_shape)
</span></span><span style="display:flex;"><span style="white-space:pre;-webkit-user-select:none;user-select:none;margin-right:0.4em;padding:0 0.4em 0 0.4em;color:#7f7f7f">12</span><span>    <span style="color:#66d9ef">return</span> quantized_matrix, centers, labels</span></span></code></pre></div>
<h2 id="conclusion">Conclusion<a href="#conclusion">
    <svg role="img" aria-labelledby="conclusion-IconTitle" fill="var(--color-primary)" height="22" viewBox="0 0 24 24" width="22" xmlns="http://www.w3.org/2000/svg">
      <title id="conclusion-IconTitle">Link to this heading</title>
      <path d="M0 0h24v24H0z" fill="none"></path>
      <path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z"></path>
    </svg>
  </a>
</h2><p>Implementing ImageGAN involved a series of carefully planned steps, from data collection to model training and optimization. The end result is a mobile application that leverages advanced GAN algorithms to create unique and captivating visuals. Stay tuned for future posts where we will discuss the deployment and performance optimization of ImageGAN on mobile devices.</p>

    </article>
    <aside>
      <div class="single-terms">
        
          
          <a class="term" href="https://ai-doge.github.io/tags/image-gan/">Image GAN</a></li>
          
          <a class="term" href="https://ai-doge.github.io/tags/colab-training/">Colab Training</a></li>
          
          <a class="term" href="https://ai-doge.github.io/tags/kmeans-weight-quant/">KMeans Weight Quant</a></li>
          
        
      </div>
      
  
  
  

  <section>
    <h2>Share</h2>
    <div class="social-links">
      <ul class="social-icons--share">
        
        
        <a href="https://twitter.com/intent/tweet?url=https%3a%2f%2fai-doge.github.io%2fposts%2fport_image_gan%2f&amp;text=Implementing%20ImageGAN%20From%20Data%20Collection%20to%20Model%20Compression" target="_blank" rel="noopener" aria-label="Share on Twitter" class="social-btn twitter">
          <li><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-twitter" width="24" height="24" viewBox="0 0 384 312" fill="var(--color-primary)"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5 0-78.8 35.3-78.8 78.8 0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3-6.7 11.6-10.6 25.2-10.6 39.6 0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1 0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4 0-12.6-.4-18.8-1.1 34.9 22.4 76.3 35.4 120.8 35.4 144.9 0 224.1-120 224.1-224.1 0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg></li>
        </a>
        
        
        
        <a href="https://www.reddit.com/submit?url=https%3a%2f%2fai-doge.github.io%2fposts%2fport_image_gan%2f" target="_blank" rel="noopener" aria-label="Share on Reddit" class="social-btn reddit">
          <li><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-reddit" width="24" height="24" viewBox="0 0 24 24" fill="var(--color-primary)"><path d="M24 11.779c0-1.459-1.192-2.645-2.657-2.645-.715 0-1.363.286-1.84.746-1.81-1.191-4.259-1.949-6.971-2.046l1.483-4.669 4.016.941-.006.058c0 1.193.975 2.163 2.174 2.163 1.198 0 2.172-.97 2.172-2.163s-.975-2.164-2.172-2.164c-.92 0-1.704.574-2.021 1.379l-4.329-1.015c-.189-.046-.381.063-.44.249l-1.654 5.207c-2.838.034-5.409.798-7.3 2.025-.474-.438-1.103-.712-1.799-.712-1.465 0-2.656 1.187-2.656 2.646 0 .97.533 1.811 1.317 2.271-.052.282-.086.567-.086.857 0 3.911 4.808 7.093 10.719 7.093s10.72-3.182 10.72-7.093c0-.274-.029-.544-.075-.81.832-.447 1.405-1.312 1.405-2.318zm-17.224 1.816c0-.868.71-1.575 1.582-1.575.872 0 1.581.707 1.581 1.575s-.709 1.574-1.581 1.574-1.582-.706-1.582-1.574zm9.061 4.669c-.797.793-2.048 1.179-3.824 1.179l-.013-.003-.013.003c-1.777 0-3.028-.386-3.824-1.179-.145-.144-.145-.379 0-.523.145-.145.381-.145.526 0 .65.647 1.729.961 3.298.961l.013.003.013-.003c1.569 0 2.648-.315 3.298-.962.145-.145.381-.144.526 0 .145.145.145.379 0 .524zm-.189-3.095c-.872 0-1.581-.706-1.581-1.574 0-.868.709-1.575 1.581-1.575s1.581.707 1.581 1.575-.709 1.574-1.581 1.574z"/></svg></li>
        </a>
        
        
                
        <a href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fai-doge.github.io%2fposts%2fport_image_gan%2f" target="_blank" rel="noopener" aria-label="Share on Facebook" class="social-btn facebook">
          <li><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-facebook" width="24" height="24" viewBox="0 0 352 352" fill="var(--color-primary)"><path d="m0 32v288c0 17.5 14.5 32 32 32h288c17.5 0 32-14.5 32-32v-288c0-17.5-14.5-32-32-32h-288c-17.5 0-32 14.5-32 32zm320 0v288h-83v-108h41.5l6-48h-47.5v-31c0-14 3.5-23.5 23.5-23.5h26v-43.5c-4.4-.6-19.8-1.5-37.5-1.5-36.9 0-62 22.2-62 63.5v36h-42v48h42v108h-155v-288z"/></svg></li>
        </a>
        
        
        
        <a href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fai-doge.github.io%2fposts%2fport_image_gan%2f&amp;source=https%3a%2f%2fai-doge.github.io%2fposts%2fport_image_gan%2f&amp;title=Implementing%20ImageGAN%20From%20Data%20Collection%20to%20Model%20Compression&amp;summary=Implementing%20ImageGAN%20From%20Data%20Collection%20to%20Model%20Compression" target="_blank" rel="noopener" aria-label="Share on LinkedIn" class="social-btn linkedin">
          <li><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-linkedin" width="24" height="24" viewBox="0 0 352 352" fill="var(--color-primary)"><path d="M0,40v272c0,21.9,18.1,40,40,40h272c21.9,0,40-18.1,40-40V40c0-21.9-18.1-40-40-40H40C18.1,0,0,18.1,0,40z M312,32 c4.6,0,8,3.4,8,8v272c0,4.6-3.4,8-8,8H40c-4.6,0-8-3.4-8-8V40c0-4.6,3.4-8,8-8H312z M59.5,87c0,15.2,12.3,27.5,27.5,27.5 c15.2,0,27.5-12.3,27.5-27.5c0-15.2-12.3-27.5-27.5-27.5C71.8,59.5,59.5,71.8,59.5,87z M187,157h-1v-21h-45v152h47v-75 c0-19.8,3.9-39,28.5-39c24.2,0,24.5,22.4,24.5,40v74h47v-83.5c0-40.9-8.7-72-56.5-72C208.5,132.5,193.3,145.1,187,157z M64,288h47.5 V136H64V288z"/></svg></li>
        </a>
        
        
        
        <a href="mailto:?subject=ai-doge%20-%20Implementing%20ImageGAN%20From%20Data%20Collection%20to%20Model%20Compression.&amp;body=Implementing%20ImageGAN%20From%20Data%20Collection%20to%20Model%20Compression%2c%20by%20ai-doge%0aHow%20to%20train%20a%20projected%20gan%20on%20landscapes%2f%20clouds%20dataset%2c%20then%20port%20it%20on%20iPhone.%0a%0ahttps%3a%2f%2fai-doge.github.io%2fposts%2fport_image_gan%2f%0a" target="_blank" class="social-btn email">
          <li><svg xmlns="http://www.w3.org/2000/svg" class="icon icon-mail" width="24" height="24" viewBox="0 0 416 288" fill="var(--color-primary)"><path d="m0 16v256 16h16 384 16v-16-256-16h-16-384-16zm347 16-139 92.5-139-92.5zm-148 125.5 9 5.5 9-5.5 167-111.5v210h-352v-210z"/></svg></li>
        </a>
      </ul>
    </div>
  </section>
  
        

  
  

        
  <section>
    <h2>Read Next</h2>
    <div class="single-next-previous">
      
      
        <a class="next" href="https://ai-doge.github.io/posts/llama-ios/">Port the LLaMa Model on iOS &raquo;</a>
      
    </div>
  </section>

      
    </aside>
  </div>
</div>

    </main><footer>
  
  <div class="section footer">
    <p class="footer-copyright">&copy; 2023 &middot; 
      <a href="https://ai-doge.github.io/">ai-doge</a>
      
        <span>&middot; Built with <a href="https://github.com/wjh18/hugo-liftoff" target="_blank" rel="noopener">Hugo Liftoff</a> theme.</span>
      
    </p>
    
      <div class="footer-socials">
        
<div class="social-links">
  <ul class="social-icons">
    
    
    <li>
      <a href="https://twitter.com/aidogeshow" target="_blank" rel="noopener" aria-label="Visit Twitter profile" class="social-btn twitter">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-twitter" width="24" height="24" viewBox="0 0 384 312" fill="var(--color-primary)"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5 0-78.8 35.3-78.8 78.8 0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3-6.7 11.6-10.6 25.2-10.6 39.6 0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1 0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4 0-12.6-.4-18.8-1.1 34.9 22.4 76.3 35.4 120.8 35.4 144.9 0 224.1-120 224.1-224.1 0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
      </a>
    </li>
    

    
    

    
    
    <li>
      <a href="https://github.com/ai-doge" target="_blank" rel="noopener" aria-label="Visit Github profile" class="social-btn github">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-github" width="24" height="24" viewBox="0 0 24 24" fill="var(--color-primary)"><path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/></svg>
      </a>
    </li>
    

    
    

    
    

    
    
  </ul>
</div>

      </div>
    
  </div>
</footer>

  





  
  
  
    
    
  




  
  
    

<script src="https://ai-doge.github.io/main.min.js"></script>



  
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WMMNNH5CCF"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-WMMNNH5CCF', { 'anonymize_ip': false });
}
</script>

</body>
</html>
